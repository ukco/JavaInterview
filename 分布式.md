# 分布式系统简介

分布式系统定义：分布式系统是一个硬件或软件组件**分布在不同的网络计算机上**，彼此之间仅仅**通过消息传递进行通信和协调**的系统。

简单来说就是一群独立计算机集合共同对外提供服务，但是对于系统的用户来说分布式架构师透明的，就像是一台计算机在提供服务一样。分布式意味着可以采用更多的普通计算机（相对于昂贵的大型机）组成分布式集群对外提供服务。计算机越多，CPU、内存、存储资源等也就越多，能够处理的并发访问量也就越大。

在分布式系统中，各个主机之间通信和协调主要通过**网络**进行，所以，分布式系统中的计算机在空间上几乎没有任何限制，这些计算机可能被放在不同的机柜上，也可能被部署在不同的机房中，还可能在不同的城市中，对于大型的网站甚至可能分布在不同的国家和地区。但是，无论空间上如何分布，一个标准的分布式系统应该具有以下几个主要特征：

*   分布性：分布式系统中的多台计算机之间在空间位置上可以随意分布，系统中的多台计算机之间没有主、从之分，即没有控制整个系统的主机，也没有受控的从机。
*   透明性：分布式系统机器上的所有资源（CPU资源、硬盘资源等）都是可以受用户调度使用的，但对用户来说这一过程是透明的，用户并不知道是有多台机器统一提供服务，而是觉得像一台计算机一样在运作
*   同一性：系统中的若干台计算机可以互相协作来完成一个共同的任务，或者说一个程序可以分布在几台计算机上并行地运行。
*   通信性：系统中任意两台计算机都可以通过通信来交换信息。

和集中式系统相比，分布式系统的性价比更高、处理能力更强、可靠性更高、也有很好的扩展性。但是，分布式在解决了网站的高并发问题的同时也带来了一些其他问题。首先，**分布式的必要条件就是网络**，这可能对性能甚至服务能力造成一定的影响。其次，一个集群中的服务器数量越多，**服务器宕机**的概率也就越大。另外，由于服务在集群中分布式部署，用户的请求只会落到其中一台机器上，所以，一旦处理不好就很容易产生**数据一致性问题**。

## 分布式与集群

**分布式（distributed）**是指在**多台不同的服务器**中部署**不同的服务模块**，通过远程调用协同工作，对外提供服务。

**集群（cluster）**是指在**多台不同的服务器**中部署**相同应用或服务模块**，构成一个集群，通过负载均衡设备对外提供服务。

# 分布式系统数据一致性

## 数据一致性

数据一致性其实是数据库系统中的概念。我们可以简单的把一致性理解为**正确性**或者**完整性**，那么数据一致性通常指关联数据之间的逻辑关系是否正确和完整。我们知道，在数据库系统中通常用**事务（访问并可能更新数据库中各种数据项的一个程序执行单元）**来保证数据的一致性和完整性。而在分布式系统中，数据一致性往往指的是由于数据的复制，不同数据节点中的数据内容是否完整并且相同。

比如在集中式系统中，有一些关键的配置信息，可以直接保存在服务器的内存中，但是在分布式系统中，如何保存这些配置信息，又如何保证所有机器上的配置信息都保持一致，又如何保证修改一个配置能够把这次修改同步到所有机器中呢？

再比如，在集中式系统中，进行一个同步操作要写同一个数据的时候，可以直接使用事务+锁来管理保证数据的ACID。但是，在分布式系统中如何保证多台机器不会同时写同一条数据呢？

除了上面提到的同一个数据的一致性，还有一种情况也可以叫做数据的一致性：比如我们在电商网站下单，需要经历扣减库存、扣减红包、扣减折扣券等一系列操作。如果库存库存扣减成功，但是红包和折扣券扣减失败的话，也可以说是数据没有保证一致性。

分布式系统为了提升可用性和性能，会通过复制技术来进行数据同步。复制机制的目的是为了保证数据的一致性。但是数据复制面临的主要难题也是如何保证多个副本之间的数据一致性。在分布式系统引入复制机制后，不同的数据节点之间由于网络延时等原因很容易产生数据不一致的情况。

## 一致性模型

1.   **强一致性**：当更新操作完成之后，任何多个后续进程或者线程的访问都会返回最新的更新过的值。这种是对用户最友好的，就是用户上一次写什么，下一次就保证能读到什么。

     但是这种实现对性能影响较大，因为这意味着，只要上次的操作没有处理完，就不能让用户读取数据。

2.   **弱一致性**：系统并不保证进程或者线程的访问都会返回最新的更新过的值。系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。但会尽可能保证在某个时间级别（比如秒级别）之后，可以让数据达到一致性状态。

3.   **最终一致性**：弱一致性的特定形式。系统保证在没有后续更新的前提下，系统最终返回上一次更新操作的值。在没有故障发生的前提下，不一致窗口的时间主要受通信延迟，系统负载和复制副本的个数影响。

## 数据多副本实现容错的情况

在分布式存储系统中经常使用多副本的方式实现容错，每一份数据都保存多个副本，这样部分副本的失效不会导致数据的丢失。**每次更新操作都需要更新数据的所有副本，使多个副本的数据保持一致**。那么问题来了，如何在一个可能出现各种故障的异步分布式系统中保证同一数据的多个副本的一致性 (Consistency) 呢？

以最简单的两副本为例，首先来看看传统的**主从同步**方式。

![image-20210904193011992](interviewImg/image-20210904193011992.png)

写请求首先发送给主副本，主副本同步更新到其它副本后返回。这种方式可以保证副本之间**数据的强一致性**，写成功返回之后从任意副本读到的数据都是一致的。但是**可用性很差**，只要任意一个副本写失败，写请求将执行失败。

![image-20210904193105941](interviewImg/image-20210904193105941.png)

如果采用**异步复制**的方式，主副本写成功后立即返回，然后在后台异步的更新其它副本。

![image-20210904193157656](interviewImg/image-20210904193157656.png)

写请求首先发送给主副本，**主副本写成功后立即返回，然后异步的更新其它副本**。这种方式可用性较好，只要主副本写成功，写请求就执行成功。但是不能保证副本之间数据的强一致性，写成功返回之后从各个副本读取到的数据不保证一致，只有主副本上是最新的数据，其它副本上的数据落后，只提供**最终一致性**。

![image-20210904193353593](interviewImg/image-20210904193353593.png)

如果出现断网导致后台异步复制失败，则主副本和其它副本将长时间不一致，其它副本上的数据一直无法更新，直到网络重新连通。

![image-20210904193422942](interviewImg/image-20210904193422942.png)

如果主副本在写请求成功返回之后和更新其它副本之前宕机失效，则会造成成功写入的数据丢失，一致性被破坏。

同步复制为最高保护模式 (Maximum Protection)，异步复制为最高性能模式 (Maximum Performance)，还有一种最高可用性模式 (Maximum Availability) 介于两者之间，在正常情况下，它和最高保护模式一样，但一旦同步出现故障，立即切换成最高性能模式。

**传统的主从同步无法同时保证数据的一致性和可用性**，此问题是典型的分布式系统中一致性和可用性不可兼得的例子，分布式系统中著名的CAP理论从理论上证明了这个问题。

## CAP理论

**一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项**。

<img src="interviewImg/image-20210904191235328.png" alt="image-20210904191235328"  />

（注：CAP中的CA和数据库事务中ACID的CA并不是同一回事儿。两者之中的A都是C都是一致性(Consistency)。CAP中的A指的是可用性（Availability），而ACID中的A指的是原子性（Atomicity)，切勿混为一谈。）

1.   Consistency 一致性

     即更新操作成功并返回客户端完成后，所有节点在同一时间的数据完全一致，所以，一致性，说的就是数据一致性。

2.   Availability 可用性

     服务一直可用，而且是正常响应时间。

     对于一个可用性的分布式系统，每一个非故障的节点必须对每一个请求作出响应。所以，一般我们在衡量一个系统的可用性的时候，都是通过停机时间来计算的。

     |          可用性分类          | 可用水平（%） | 年可容忍停机时间 |
     | :--------------------------: | :-----------: | :--------------: |
     |          容错可用性          |    99.9999    |      <1 min      |
     |          极高可用性          |    99.999     |      <5 min      |
     | 具有故障自动恢复能力的可用性 |     99.99     |     <53 min      |
     |           高可用性           |     99.9      |      <8.8h       |
     |          商品可用性          |      99       |    <43.8 min     |

     通常我们描述一个系统的可用性可以达到5个9，意思就是说他的可用水平是99.999%，即全年停机时间不超过 `(1-0.99999)*365*24*60 = 5.256 min`，这是一个极高的要求。

3.   Partition Tolerance 分区容错性

     分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。

理解CAP理论最简单的方式是想象两个副本处于分区两侧，即两个副本之间的网络断开，不能通信。

-   如果允许其中一个副本更新，则会导致数据不一致，即丧失了C性质。
-   如果为了保证一致性，将分区某一侧的副本设置为不可用，那么又丧失了A性质。
-   除非两个副本可以互相通信，才能既保证C又保证A，这又会导致丧失P性质。

一般来说使用网络通信的分布式系统，无法舍弃P性质，在分布式环境下，网络分区是一个自然的事实。因为分区是必然的，所以如果舍弃P，意味着要舍弃分布式系统。那也就没有必要再讨论CAP理论了。我们以系统满足P为前提论述了无法同时满足C和A，那么就只能在一致性和可用性上做一个艰难的选择。

当然，CAP理论的“三选二”的公式一直存在着误导性，它会过分简单化各性质之间的相互关系：

-   首先，由于分区很少发生，那么在系统不存在分区的情况下没什么理由牺牲C或A。
-   其次，C与A之间的取舍可以在同一系统内以非常细小的粒度反复发生，而每一次的决策可能因为具体的操作，乃至因为牵涉到特定的数据或用户而有所不同。
-   最后，**这三种性质都可以在程度上衡量，并不是非黑即白的有或无**。可用性显然是在0%到100%之间连续变化的，一致性分很多级别，连分区也可以细分为不同含义，如系统内的不同部分对于是否存在分区可以有不一样的认知。

所以一致性和可用性并不是水火不容，非此即彼的。Paxos、Raft等分布式一致性算法就是在一致性和可用性之间做到了很好的平衡的见证。

## BASE理论

BASE理论是对CAP理论的延伸，核心思想是即使无法做到强一致性（Strong Consistency，CAP的一致性就是强一致性），但应用可以**采用适合的方式达到最终一致性**（Eventual Consitency）。

BASE是指基本可用（Basically Available）、软状态（ Soft State）、最终一致性（ Eventual Consistency）。

1.   Basically Available 基本可用

     基本可用是指分布式系统在出现故障的时候，允许损失部分可用性，即保证核心可用。

     （电商大促时，为了应对访问量激增，部分用户可能会被引导到降级页面，服务层也可能只提供降级服务。这就是损失部分可用性的体现。）

2.   Soft State 软状态

     软状态是指允许系统存在**中间状态**，而该中间状态不会影响系统整体可用性。分布式存储中一般一份数据至少会有三个副本，**允许不同节点间副本同步的延时就是软状态的体现**。mysql replication的异步复制也是一种体现。

3.   Eventual Consistency 最终一致性

     最终一致性是指**系统中的所有数据副本经过一定时间后，最终能够达到一致的状态**。弱一致性和强一致性相反，最终一致性是弱一致性的一种特殊情况。

## 分布式事务

分布式事务是指会涉及到**操作多个数据库的事务**。其实就是将对同一库事务的概念扩大到了对多个库的事务。目的是**为了保证分布式系统中的数据一致性**。分布式事务处理的关键是必须有一种方法可以知道事务在任何地方所做的所有动作，提交或回滚事务的决定必须产生统一的结果（全部提交或全部回滚）。

在分布式系统中，各个节点之间在物理上相互独立，通过网络进行沟通和协调。**由于存在事务机制，可以保证每个独立节点上的数据操作可以满足ACID**。但是，相互独立的节点之间无法准确的知道其他节点中的事务执行情况。所以从理论上讲，**两台机器理论上无法达到一致的状态**。如果想让分布式部署的多台机器中的数据保持一致性，那么就要保证在所有节点的数据写操作，**要么全部都执行，要么全部的都不执行**。但是，一台机器在执行本地事务的时候无法知道其他机器中的本地事务的执行结果。所以它也就不知道本次事务到底应该commit还是 roolback。所以，常规的解决办法就是引入一个“**协调者**”的组件来**统一调度所有分布式节点的执行**。

### XA规范

~~X/Open 组织（即现在的 Open Group ）定义了分布式事务处理模型。 X/Open DTP 模型（ 1994 ）包括应用程序（ AP ）、事务管理器（ TM ）、资源管理器（ RM ）、通信资源管理器（ CRM ）四部分。一般，常见的事务管理器（ TM ）是交易中间件，常见的资源管理器（ RM ）是数据库，常见的通信资源管理器（ CRM ）是消息中间件。~~

通常把一个数据库内部的事务处理，如对多个表的操作，作为**本地事务**看待。数据库的事务处理对象是本地事务，而分布式事务处理的对象是**全局事务**。  所谓**全局事务，是指分布式事务处理环境中，多个数据库可能需要共同完成一个工作，这个工作即是一个全局事务**，例如，一个事务中可能更新几个不同的数据库。对数据库的操作发生在系统的各处但必须**全部被提交或回滚**。此时一个数据库对自己内部所做操作的提交不仅依赖本身操作是否成功，还要依赖于全局事务相关的其它数据库的操作是否成功。如果任一数据库的任一操作失败，则参与此事务的所有数据库所做的所有操作都必须回滚。   一般情况下，*某一数据库无法知道其它数据库在做什么*，因此，在一个 DTP（分布式事务模型） 环境中，事务管理器（交易中间件）是必需的，由它**通知和协调**相关数据库的提交或回滚。而一个数据库只将其自己所做的操作（可恢复）影射到全局事务中。 

>   XA 就是 X/Open DTP 定义的交易中间件与数据库之间的接口规范（即接口函数），交易中间件用它来通知数据库事务的开始、结束以及提交、回滚等。 XA 接口函数由数据库厂商提供。 

## 二阶段提交协议（2PC）

可以说二阶段提交其实就是实现XA分布式事务的关键（确切地说：二阶段提交主要保证了分布式事务的**原子性**：即所有结点要么全做要么全不做）。

二阶段提交（Two-phase Commit）是指，在计算机网络以及数据库领域内，**为了使基于分布式系统架构下的所有节点在进行事务提交时保持一致性而设计的一种算法(Algorithm)**。通常，二阶段提交也被称为是一种协议(Protocol))。在分布式系统中，每个节点虽然可以知晓自己的操作时成功或者失败，却无法知道其他节点的操作的成功或失败。当一个事务跨越多个节点时，为了保持事务的ACID特性，需要引入一个作为**协调者**的组件来统一掌控所有节点(称作**参与者**)的操作结果并最终指示这些节点是否要把操作结果进行真正的提交(比如将更新后的数据写入磁盘等等)。因此，二阶段提交的算法思路可以概括为：**参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作**。

所谓的两个阶段是指：第一阶段：**准备阶段(投票阶段)**和第二阶段：**提交阶段（执行阶段）**。

*   **准备阶段（投票阶段）**

    协调者给每个参与者发送**Prepare消息**，每个参与者要么直接返回失败(如权限验证失败)，要么在本地执行事务，写本地的**redo和undo日志**，但不提交，到达一种“万事俱备，只欠东风”的状态。

    可以进一步将准备阶段分为以下三个步骤：

    >   1）协调者节点向所有参与者节点询问是否可以执行提交操作(vote)，并开始等待各参与者节点的响应。
    >
    >   2）参与者节点检查事务权限，执行询问发起为止的所有事务操作，并将Undo信息和Redo信息写入日志。（注意：若成功这里其实每个参与者已经执行了事务操作）。
    >
    >   3）各参与者节点响应协调者节点发起的询问。如果**参与者节点的事务操作实际执行成功**，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。

*   **提交阶段（执行阶段）**

    如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送**回滚(Rollback)消息**；否则，发送**提交(Commit)消息**；参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理过程中使用的锁资源。(注意：**必须在最后阶段释放锁资源**)。

接下来分两种情况分别讨论提交阶段的过程。

**情况1**：当协调者节点从所有参与者节点获得的相应消息都为”同意”时：

![image-20210904224621094](interviewImg/image-20210904224621094.png)

1）协调者节点向所有参与者节点发出”正式提交(commit)”的请求。
2）参与者节点正式完成操作，并释放在整个事务期间内占用的资源。
3）参与者节点向协调者节点发送”完成”消息。
4）协调者节点受到所有参与者节点反馈的”完成”消息后，完成事务。

**情况2**：如果任一参与者节点在第一阶段返回的响应消息为”中止”，或者协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时：

![image-20210904224913839](interviewImg/image-20210904224913839.png)

1）协调者节点向所有参与者节点发出”回滚操作(rollback)”的请求。
2）参与者节点利用之前写入的Undo信息执行回滚，并释放在整个事务期间内占用的资源。
3）参与者节点向协调者节点发送”回滚完成”消息。
4）协调者节点受到所有参与者节点反馈的”回滚完成”消息后，取消事务。

不管最后结果如何，第二阶段都会结束当前事务。

### 二阶段提交的缺点

二阶段提交看起来确实能够提供原子性的操作，但是不幸的事，二阶段提交还是有几个缺点的：

![image-20210904225131726](interviewImg/image-20210904225131726.png)

1、**同步阻塞问题**。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。

2、**单点故障**。由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）

3、**数据不一致**。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这会导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据不一致性的现象。

4、二阶段无法解决的问题--**极限情况下,对某一事务的不确定性**：协调者在发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否已经被提交。

由于二阶段提交存在着诸如**同步阻塞、单点问题、脑裂**等缺陷，所以，研究者们在二阶段提交的基础上做了改进，提出了三阶段提交。

## 三阶段提交协议（3PC）

三阶段提交（Three-phase commit），也叫三阶段提交协议（Three-phase commit protocol），是二阶段提交（2PC）的改进版本。

![image-20210904225651291](interviewImg/image-20210904225651291.png)

与两阶段提交不同的是，三阶段提交有两个改动点。

1.   引入**超时机制**。同时在协调者和参与者中都引入超时机制。
2.   在第一阶段和第二阶段之前引入**准备阶段，**保证了在最后提交阶段之前各参与节点状态的一致。

也就是说，除了引入超时机制之外，3PC**把2PC的准备阶段再次一分为二**，这样三阶段提交就有`CanCommit`、`PreCommit`、`DoCommit`三个阶段。

>   **为什么要把投票阶段一分为二？**
>
>   **假设有1个协调者，9个参与者。其中有一个参与者不具备执行该事务的能力。**
>   协调者发出Prepare消息之后，其余参与者都将资源锁住，执行事务，写入undo和redo日志。
>   协调者收到相应之后，发现有一个参与者不能参与。所以，又出一个rollback消息。其余8个参与者，又对消息进行回滚。这样子，是不是做了很多无用功？
>   所以**，**引入Can-Commit阶段，**主要是为了在预执行之前，保证所有参与者都具备可执行条件，从而减少资源浪费。**

![image-20210904230728172](interviewImg/image-20210904230728172.png)

*   **CanCommit阶段**

    3PC的CanCommit阶段其实和2PC的准备阶段很像。协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应。

    >   **1.事务询问** 协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。
    >
    >   **2.响应反馈** 参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态。否则反馈No

*   **PreCommit阶段**

    本阶段协调者会根据第一阶段的询盘结果采取相应操作，询盘结果主要有两种：

    **情况1-**假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务的预执行：

    >   **1.发送预提交请求** 协调者向参与者发送PreCommit请求，并进入Prepared阶段。
    >
    >   **2.事务预提交** 参与者接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中。
    >
    >   **3.响应反馈** 如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。

    **情况2-**假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。具体步骤如下：

    >   **1.发送中断请求** 协调者向所有参与者发送abort请求。
    >
    >   **2.中断事务** 参与者收到来自协调者的abort请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。

    ![image-20210904231343083](interviewImg/image-20210904231343083.png)

*   **DoCommit阶段**

    该阶段进行真正的事务提交，也可以分为以下两种情况。

    **情况1-执行提交**

    针对第一种情况，协调者向各个参与者发起事务提交请求，具体步骤如下：

    >   1.   **发送提交请求** 协调接收到参与者发送的ACK响应，那么它将从预提交状态进入到提交状态。并向所有参与者发送doCommit请求。
    >   2.   **事务提交** 参与者接收到doCommit请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。
    >   3.   **响应反馈** 事务提交完之后，向协调者发送Ack响应。
    >   4.   **完成事务** 协调者接收到所有参与者的ack响应之后，完成事务。

    ![image-20210904231629384](interviewImg/image-20210904231629384.png)

    **情况2-中断事务**

    协调者没有接收到参与者发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。具体步骤如下：

    >   1.   **发送中断请求** 协调者向所有参与者发送事务abort通知。
    >   2.    **事务回滚** 参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。
    >   3.    **反馈结果** 参与者完成事务回滚之后，向协调者发送ACK消息
    >   4.    **中断事务** 协调者接收到参与者反馈的ACK消息之后，执行事务的中断。

    ![image-20210904231805754](interviewImg/image-20210904231805754.png)

注：在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，**会在等待超时之后，继续进行事务的提交**。（当进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求，也即协调者产生PreCommit请求的前提条件是它在第二阶段开始之前，收到所有参与者的CanCommit响应都是Yes。（一旦参与者收到了PreCommit，意味它知道大家其实都同意修改了）所以，一句话概括就是，**当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到commit或者abort响应，但是它有理由相信：成功提交的几率很大**。）

### 2PC与3PC的区别

相对于2PC，**3PC主要解决的单点故障问题，并减少阻塞**，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持有事务资源并处于阻塞状态。

但是这种机制**也会导致数据一致性问题**。由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。

了解了2PC和3PC之后，我们可以发现，无论是二阶段提交还是三阶段提交都无法彻底解决分布式的一致性问题。

**世上只有一种一致性算法，那就是Paxos**，所有其他一致性算法都是Paxos算法的不完整版。

## Paxos算法



















































