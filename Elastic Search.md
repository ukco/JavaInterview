# es召回率、准确率

召回率也叫查全率，是检索出的相关文档数和文档库中所有的相关文档数的比率。衡量的是检索结果的查全率。

准确率也叫精度，是检索出的相关文档数与检索出的文档总数的比率。衡量的是检索结果的查准率。

# ES在数据量很大的情况下（数十亿级别）如何提高查询效率？

在es中，如果数据量很大的时候，特别是有几亿条数据的时候，跑个搜索可能一下子要5~10s。后面反而就快了，可能就几百毫秒。

* 性能优化杀手锏——filesystem cache

  我们在往es里写数据的时候，实际上都写到磁盘文件里去了，查询的时候，操作系统会将磁盘文件里的数据自动缓存到filesystem cache里面去。

  <img src="Elastic Search.assets/image-20210420101813305.png" alt="image-20210420101813305" style="zoom:80%;" />

  es 的搜索引擎严重依赖于底层的 `filesystem cache` ，如果给 `filesystem cache` 更多的内存，尽量让内存可以容纳所有的 `idx segment file `索引数据文件，那么搜索的时候就基本都是走内存的，性能会非常高。如果走磁盘一般肯定上秒，搜索性能绝对是秒级别的，1 秒、5 秒、10 秒。但如果是走 `filesystem cache` ，是走纯内存的，那么一般来说性能比走磁盘要高一个数量级，基本上就是毫秒级的，从几毫秒到几百毫秒不等。

  ~~例如：某个公司 es 节点有 3 台机器，每台机器内存64G，总内存就是 `64 * 3 = 192G` 。每台机器给 es jvm heap 是 `32G` ，那么剩下来留给 `filesystem cache` 的就是每台机器才 `32G` ，总共集群里给 `filesystem cache` 的就是 `32 * 3 = 96G` 内存。而此时，整个磁盘上索引数据文件，在 3 台机器上一共占用了 `1T` 的磁盘容量，es 数据量是 `1T` ，那么每台机器的数据量是 `300G` 。这样性能好吗？ `filesystem cache` 的内存才 100G，十分之一的数据可以放内存，其他的都在磁盘，然后执行搜索操作，大部分操作都是走磁盘，性能肯定差。~~

  **要想让 es 性能要好，最佳的情况下，就是机器的内存，至少可以容纳总数据量的一半。**

  最佳情况就是，仅仅在 es 中就存少量的数据，就是要**用来搜索的那些索引**，如果内存留给 `filesystem cache` 的是 100G，那么你就将索引数据控制在 `100G` 以内，这样的话，你的数据几乎全部走内存来搜索，性能非常之高，一般可以在 1 秒以内。

  比如说你现在有一行数据。 `id,name,age ....` 30 个字段。但是你现在搜索，只需要根据 `id,name,age` 三个字段来搜索。如果往 es 里写入一行数据所有的字段，就会导致说 `90%` 的数据是不用来搜索的，结果硬是占据了 es 机器上的 `filesystem cache` 的空间，单条数据的数据量越大，就会导致 `filesystem cahce` 能缓存的数据就越少。其实，仅仅写入 es 中要用来检索的**少数几个字段**就可以了，比如说就写入 es `id,name,age` 三个字段，然后你可以把其他的字段数据存在 mysql/hbase 里，我们一般是建议用 `es + hbase` 这么一个架构。

  hbase 的特点是**适用于海量数据的在线存储**，就是对 hbase 可以写入海量数据，但是不要做复杂的搜索，做很简单的一些根据 id 或者范围进行查询的这么一个操作就可以了。从 es 中根据 name 和 age 去搜索，拿到的结果可能就 20 个 `doc id` ，然后根据 `doc id` 到 hbase 里去查询每个 `doc id` 对应的**完整的数据**，给查出来，再返回给用户。

  写入 es 的数据最好小于等于，或者是略微大于 es 的 filesystem cache 的内存容量。从 es 检索可能花费 20ms，然后再根据 es 返回的 id 去 hbase 里查询，查 20 条数据，可能也就耗费个 30ms，可能你原来那么玩儿，1T 数据都放 es，会每次查询都是 5~10s，现在可能性能就会很高，每次查询就是 50ms。

* 数据预热

  假如说，哪怕是你就按照上述的方案去做了，es 集群中每个机器写入的数据量还是超过了 `filesystem cache` 一倍，比如说你写入一台机器 60G 数据，结果 `filesystem cache` 就 30G，还是有 30G 数据留在了磁盘上。

  其实可以做**数据预热**

  举个例子，拿微博来说，你可以把一些大 V，平时看的人很多的数据，你自己提前后台搞个系统，每隔一会儿，自己的后台系统去搜索一下热数据，刷到 `filesystem cache` 里去，后面用户实际上来看这个热数据的时候，他们就是直接从内存里搜索了，很快。或者是电商，你可以将平时查看最多的一些商品，比如说 iphone 8，热数据提前后台搞个程序，每隔 1 分钟自己主动访问一次，刷到 `filesystem cache` 里去。

  对于那些比较热的、经常会有人访问的数据，最好**做一个专门的缓存预热子系统**，就是对热数据每隔一段时间，就提前访问一下，让数据进入 `filesystem cache` 里面去。这样下次别人访问的时候，性能一定会好很多。

* 冷热分离

  es 可以做类似于 mysql 的水平拆分，就是说将大量的访问很少、频率很低的数据，单独写一个索引，然后将访问很频繁的热数据单独写一个索引。最好是将**冷数据写入一个索引中，然后热数据写入另外一个索引中**，这样可以确保热数据在被预热之后，尽量都让他们留在 `filesystem os cache` 里，**别让冷数据给冲刷掉**。

  假设有 6 台机器，2 个索引，一个放冷数据，一个放热数据，每个索引 3 个 shard。3 台机器放热数据 index，另外 3 台机器放冷数据 index。然后这样的话，你大量的时间是在访问热数据 index，热数据可能就占总数据量的 10%，此时数据量很少，几乎全都保留在 `filesystem cache` 里面了，就可以确保热数据的访问性能是很高的。但是对于冷数据而言，是在别的 index 里的，跟热数据 index 不在相同的机器上，大家互相之间都没什么联系了。如果有人访问冷数据，可能大量数据是在磁盘上的，此时性能差点，就 10% 的人去访问冷数据，90% 的人在访问热数据。

* document模型设计

  对于 MySQL，我们经常有一些复杂的关联查询。但在es 里面的复杂的关联查询尽量别用，一旦用了性能一般都不太好。最好是先在es系统外面就完成关联，将关联好的数据直接写入 es 中。搜索的时候，就不需要利用 es 的搜索语法来完成 join 之类的关联搜索了。

  document 模型设计是非常重要的，很多操作，尽量在 document 模型设计的时候，即写入的时候就完成。另外对于一些太复杂的操作，比如 join/nested/parent-child 搜索都要尽量避免，性能都很差的。

* 分页性能优化

  假如你每页是 10 条数据，你现在要查询第 100 页，实际上是会把每个 shard 上存储的前 1000 条数据都查到一个协调节点上，如果你有个 5 个 shard，那么就有 5000 条数据，接着协调节点对这 5000 条数据进行一些合并、处理，再获取到最终第 100 页的 10 条数据。

  分布式的，你要查第 100 页的 10 条数据，不可能说从 5 个 shard，每个 shard 就查 2 条数据，最后到协调节点合并成 10 条数据吧？你**必须**得从每个 shard 都查 1000 条数据过来，然后根据你的需求进行排序、筛选等等操作，最后再次分页，拿到里面第 100 页的数据。你翻页的时候，翻的越深，每个 shard 返回的数据就越多，而且协调节点处理的时间越长，非常耗时。所以用 es 做分页的时候，你会发现越翻到后面，就越慢。

  **不允许深度分页（默认深度分页性能很差）**

# es选主过程

## 7.X之前的选主流程（ZenDiscovery）

采用Bully算法，它假定所有节点都有一个唯一的ID，**使用该ID对节点进行排序**。**任何时候的当前Leader都是参与集群的最高ID节点**。该算法的优点是易于实现。但是，当拥有最大ID的节点处于不稳定状态的场景下会有问题。例如，Master负载过重而假死，集群拥有第二大ID的节点被选为新主，这时原来的Master恢复，再次被选为新主，然后又假死。

ES 通过**推迟选举**，直到当前的 Master 失效来解决上述问题，**只要当前主节点不挂掉，就不重新选主**。但是容易产生脑裂（双主），为此，再通过“法定得票人数过半”解决脑裂问题

只有一个 Leader将当前版本的全局集群状态推送到每个节点。 ZenDiscovery（默认）过程就是这样的:

* 每个节点计算最高的已知节点ID，并向该节点发送leader投票
* 如果一个节点收到足够多的票数，并且该节点也为自己投票，那么它将扮演leader的角色，开始发布集群状态。
* 所有节点都会参与选举，并参与投票，但是只有有资格成为 master 的节点的投票才有效.

有多少选票赢得选举的定义就是所谓的**法定人数**。 在弹性搜索中，法定大小是一个可配置的参数。 （一般配置成：**可以成为master节点数n/2+1**）

![image-20210914141633614](interviewImg\image-20210914141633614.png)

### 什么时候选主

1. 集群启动
2. Master失效，非 Master 节点运行的 MasterFaultDetection 检测到 Master 失效,在其注册的 listener 中执行 handleMasterGone，执行 rejoin 操作，重新选主。注意，**即使一个节点认为 Master 失效也会进入选主流程**

### Bully算法缺陷

#### Master假死

Master节点承担的职责负载过重的情况下，可能无法即时对组内成员作出响应，这种便是假死。例如一个集群中的Master假死，其他节点开始选主，刚刚选主成功，原来的Master恢复了，因为**原来Master节点的Id优先级最高，又开始一轮选主，重新把原来Master选举为Master**。

为了解决这个问题，当Master节点假死的时候会去探测是是不是真的挂了，如果不是会继续**推迟选主**过程。

#### 脑裂

当发生**网络分区故障**就会发生脑裂，就会出现双主情况，那么这个时候是十分危险的因为两个新形成的集群会同时索引和修改集群的数据，导致数据不一致。

解决方案：法定投票人数过半。有一个minimum_master_nodes设置，集群中节点必须大于这个数字才进行选主，否则不进行。

## 7.X之后的选主流程（Raft算法）

7.X之后的ES，采用一种新的选主算法，实际上是 Raft 的实现，但并非严格按照 Raft 论文实现，而是做了一些调整。

Raft是工程上使用较为广泛分布式**共识协议**，是多个节点对某个事情达成一致的看法，即使是在部分节点故障、网络延时、网络分区的情况下。

Raft 将问题分解为：Leader 选举，日志复制，安全性，将这三个问题独立思考。

在 Raft 中，节点可能的状态有三种，其转换关系如下：

![image-20210914143435611](interviewImg\image-20210914143435611.png)

正常情况下，集群中只有一个 Leader，其他节点全是 Follower 。**Follower 都是被动接收请求，从不发送主动任何请求**。Candidate 是从 Follower 到 Leader的中间状态。

Raft 中引入**任期（term）**的概念，**每个 term 内最多只有一个 Leader**。term在 Raft 算法中充当**逻辑时钟**的作用。服务器之间通信的时候会携带这个 term，**如果节点发现消息中的 term小于自己的 term，则拒绝这个消息，如果大于本节点的 term，则更新自己的 term**。**如果一个 Candidate 或者 Leader 发现自己的任期号过期了，它会立即回到 Follower 状态**。

Raft 选举流程为：

1. 增加节点本地的 current term ，切换到Candidate状态
2. 投自己一票
3. 并行给其他节点发送 RequestVote RPC（让大家投他）。

然后等待其他节点的响应，会有如下三种结果：

1. 如果接收到大多数服务器的选票，那么就变成Leader。成为Leader后，向其他节点发送心跳消息来确定自己的地位并阻止新的选举。
2. 如果收到了别人的投票请求，且别人的term比自己的大，那么候选者退化为follower
3. 如果选举过程超时，再次发起一轮选举

通过下面的约束来确定唯一 Leader（选举安全性）：

1. 同一任期内，每个节点只有一票
2. 得票(日志信息不旧于Candidate的日志信息)过半则当选为 Leader

成为 Leader 后，向其他节点发送心跳消息来确定自己的地位并阻止新的选举。

当同时满足以下条件时，Follower同意投票：

1. RequestVote请求包含的term大于等于当前term
2. 日志信息不旧于Candidate的日志信息
3. first-come-first-served 先来先得

#### ES实现的Raft算法选主流程

ES 实现的 Raft 中，选举流程与标准的有很多区别：

1. 初始为 Candidate状态
2. 执行 PreVote 流程，并拿到 maxTermSeen
3. 准备 RequestVote 请求（StartJoinRequest），基于maxTermSeen，将请求中的 term 加1（尚未增加节点当前 term）
4. 并行发送 RequestVote，异步处理。目标节点列表中包括本节点。

ES 实现中，**候选人不先投自己，而是直接并行发起 RequestVote**，这相当于候选人有投票给其他候选人的机会。这样的好处是可以在一定程度上避免3个节点同时成为候选人时，都投自己，无法成功选主的情况。

ES**不限制每个节点在某个 term 上只能投一票，节点可以投多票**，这样会产生**多主**的情况：

![image-20210914144312229](interviewImg\image-20210914144312229.png)

Node2被选为主：收到的投票为：Node2,Node3
Node3被选为主：收到的投票为：Node3,Node1

对于这种情况，ES 的处理是**让最后当选的 Leader 成功**。作为 Leader，如果收到 RequestVote请求，他会无条件退出 Leader 状态。在本例中，Node2先被选为主，随后他收到 Node3的 RequestVote 请求，那么他退出 Leader 状态，切换为Candidate，并同意向发起 RequestVote候选人投票。因此最终 Node3成功当选为 Leader。

#### 动态维护参选节点列表

在此之前，我们讨论的前提是在集群节点数量不变的情况下，现在考虑集群扩容，缩容，**节点临时或永久离线时是如何处理的**。在7.x 之前的版本中，用户需要手工配置 minimum_master_nodes，来明确告诉集群过半节点数应该是多少，并在集群扩缩容时调整他。现在，集群可以自行维护。

在取消了discovery.zen.minimum_master_nodes配置后，ES 如何判断多数？是自己计算和维护minimum_master_nodes值么？不，现在的做法不再记录“quorum” 的具体数值，取而代之的是**记录一个节点列表**，这个列表中**保存所有具备 master 资格的节点**（有些情况下不是这样，例如集群原本只有1个节点，当增加到2个的时候，这个列表维持不变，因为如果变成2，当集群任意节点离线，都会导致无法选主。这时如果再增加一个节点，集群变成3个，这个列表中就会更新为3个节点），称为 VotingConfiguration，他会持久化到集群状态中。

**在节点加入或离开集群之后，Elasticsearch会自动对VotingConfiguration做出相应的更改，以确保集群具有尽可能高的弹性**。在从集群中删除更多节点之前，等待这个调整完成是很重要的。你不能一次性停止半数或更多的节点。（感觉大面积缩容时候这个操作就比较感人了，一部分一部分缩）

默认情况下，ES 自动维护VotingConfiguration，有新节点加入的时候比较好办，但是当有节点离开的时候，他可能是暂时的重启，也可能是永久下线。你也可以人工维护 VotingConfiguration，配置项为：cluster.auto_shrink_voting_configuration，当你选择人工维护时，有节点永久下线，需要通过 voting exclusions API 将节点排除出去。如果使用默认的自动维护VotingConfiguration，也可以使用 voting exclusions API 来排除节点，例如一次性下线半数以上的节点。

如果在维护VotingConfiguration时发现节点数量为**偶数**，ES 会**将其中一个排除在外，保证 VotingConfiguration是奇数**。因为当是偶数的情况下，网络分区将集群划分为大小相等的两部分，那么两个子集群都无法达到“多数”的条件。

# ES启动过程

当Elasticsearch节点启动时，它使用发现（discovery）模块来发现同一个集群中的其他节点（这里的关键是配置文件中的集群名称）并与它们连接。默认情况下，Elasticsearch节点会向网络中发送广播请求，以找到拥有相同集群名称的其他节点。

![image-20210914153520047](interviewImg\image-20210914153520047.png)

集群中有一个节点被选为主（master）节点。该节点**负责集群的状态管理以及在集群拓扑变化时做出反应，分发索引分片至集群的响应节点上去**。

管理节点读取集群的状态信息，如有必要，它会进行恢复处理。在该阶段，管理节点会检查有哪些索引分片，并决定哪些分片将做主分片。此后，整个集群进入黄色状态。这意味着集群可以执行查询，但是系统的吞吐量以及各种可能的状况是未知的（可以简单理解为所有的主分片已经被分配了，但是副本没有被分配）。

下面的事情就是寻找到冗余的分片用作副本分片。如果某个主分片的副本数过少，管理节点将决定基于某个主分片创建分片和副本。如果一切顺利，集群将进入绿色状态（这意味着所有主分片及副本分片均已分配好）。

# 故障检测

集群正常工作时，管理节点会监控所有可用节点，检查它们是否正在工作。如果任何节点在预定义的超时时间内不响应，则认为该节点已经断开，然后错误处理过程开始启动。这意味着可能要在集群 — 分片之间重新做平衡，选择新的主节点，对每个丢失的主分片，将从它的副本分片中选出来作为新的主分片。

为了描述故障检测（failure detection）是如何工作的，我们用一个只有3个节点的集群作为例子，将会有一个管理节点，两个数据节点。

管理节点会发送ping请求至其他节点，然后等待响应。如果没有响应，则该节点会被从集群中移出去。相反地，所有节点也会向主节点发送ping请求来检查主节点是否在正常工作。节点之间的相互探测如下图所示。

![image-20210914153740553](interviewImg\image-20210914153740553.png)

# 分布式文档存储

## ES索引文档

当索引一个文档的时候，文档会被存储到一个主分片中。 Elasticsearch 如何知道一个文档应该存放到哪个分片中呢？当我们创建文档时，它如何决定这个文档应当被存储在分片 `1` 还是分片 `2` 中呢？

首先这肯定不会是随机的，否则将来要获取文档的时候我们就不知道从何处寻找了。实际上，这个过程是根据下面这个公式决定的：

```
shard = hash(routing) % number_of_primary_shards
```

`routing` 是一个可变值，默认是文档的 `_id` ，也可以设置成一个自定义的值。 `routing` 通过 hash 函数生成一个数字，然后这个数字再除以 `number_of_primary_shards` （主分片的数量）后得到 **余数** 。这个分布在 `0` 到 `number_of_primary_shards-1` 之间的余数，就是我们所寻求的文档所在分片的位置。

这就解释了为什么我们要在创建索引的时候就确定好主分片的数量 并且永远不会改变这个数量：因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。

## 主分片和副本分片如何交互

为了说明目的, 我们 假设有一个集群由三个节点组成。 它包含一个叫 `blogs` 的索引，有两个主分片，每个主分片有两个副本分片。相同分片的副本不会放在同一节点，所以我们的集群看起来像 [Figure 8, “有三个节点和一个索引的集群”]。

![image-20210914154313719](interviewImg\image-20210914154313719.png)

我们可以发送请求到集群中的任一节点。**每个节点都有能力处理任意请求。 每个节点都知道集群中任一文档位置，所以可以直接将请求转发到需要的节点上**。 在下面的例子中，将所有的请求发送到 `Node 1` ，我们将其称为 **协调节点(coordinating node)** 。

> 当发送请求的时候， 为了扩展负载，更好的做法是轮询集群中所有的节点。

## 新建、索引和删除文档

新建、索引和删除 请求都是 *写* 操作， **必须在主分片上面完成之后才能被复制到相关的副本分片**，如下图所示 [Figure 9, “新建、索引和删除单个文档”]。

![image-20210914154602132](interviewImg\image-20210914154602132.png)

以下是在主副分片和任何副本分片上面 成功新建，索引和删除文档所需要的步骤顺序：

1. 客户端向 `Node 1` 发送新建、索引或者删除请求。
2. 节点使用文档的 `_id` 确定文档属于分片 0 。请求会被转发到 `Node 3`，因为分片 0 的主分片目前被分配在 `Node 3` 上。
3. `Node 3` 在主分片上面执行请求。如果成功了，它将请求并行转发到 `Node 1` 和 `Node 2` 的副本分片上。一旦所有的副本分片都报告成功, `Node 3` 将向协调节点报告成功，协调节点向客户端报告成功。

在客户端收到成功响应时，文档变更已经在主分片和所有副本分片执行完成，变更是安全的。

有一些可选的请求参数允许您影响这个过程，可能以数据安全为代价提升性能。这些选项很少使用，因为Elasticsearch已经很快，但是为了完整起见，在这里阐述如下：

**`consistency`**

consistency，即一致性。在默认设置下，即使仅仅是在试图执行一个_写_操作之前，主分片都会要求 必须要有 *规定数量(quorum)*（或者换种说法，也即必须要有大多数）的分片副本处于活跃可用状态，才会去执行_写_操作(其中分片副本可以是主分片或者副本分片)。这是为了避免在发生网络分区故障（network partition）的时候进行_写_操作，进而导致数据不一致。_规定数量_即：

```
int( (primary + number_of_replicas) / 2 ) + 1
```

`consistency` 参数的值可以设为 `one` （只要主分片状态 ok 就允许执行_写_操作）,`all`（必须要主分片和所有副本分片的状态没问题才允许执行_写_操作）, 或 `quorum` 。默认值为 `quorum` , 即大多数的分片副本状态没问题就允许执行_写_操作。

注意，*规定数量* 的计算公式中 `number_of_replicas` 指的是在索引设置中的设定副本分片数，而不是指当前处理活动状态的副本分片数。如果你的索引设置中指定了当前索引拥有三个副本分片，那规定数量的计算结果即：

```
int( (primary + 3 replicas) / 2 ) + 1 = 3
```

如果此时你只启动两个节点，那么处于活跃状态的分片副本数量就达不到规定数量，也因此您将无法索引和删除任何文档。

**`timeout`**

如果没有足够的副本分片会发生什么？ Elasticsearch会等待，希望更多的分片出现。默认情况下，它最多等待1分钟。 如果你需要，你可以使用 `timeout` 参数 使它更早终止： `100` 100毫秒，`30s` 是30秒。

> 新索引默认有 `1` 个副本分片，这意味着为满足 `规定数量` *应该* 需要两个活动的分片副本。 但是，这些默认的设置会阻止我们在单一节点上做任何事情。为了避免这个问题，要求只有当 `number_of_replicas` 大于1的时候，规定数量才会执行。

## 取回一个文档

可以从主分片或者从其它任意副本分片检索文档 ，如下图所示 [Figure 10, “取回单个文档”]。

![image-20210914155125170](interviewImg\image-20210914155125170.png)

以下是从主分片或者副本分片检索文档的步骤顺序：

1、客户端向 `Node 1` 发送获取请求。

2、节点使用文档的 `_id` 来确定文档属于分片 `0` 。分片 `0` 的副本分片存在于所有的三个节点上。 在这种情况下，它将请求转发到 `Node 2` 。

3、`Node 2` 将文档返回给 `Node 1` ，然后将文档返回给客户端。

在处理读取请求时，协调结点在每次请求的时候都会**通过轮询所有的副本分片来达到负载均衡**。

在文档被检索时，已经被索引的文档可能已经存在于主分片上但是还没有复制到副本分片。 在这种情况下，副本分片可能会报告文档不存在，但是主分片可能成功返回文档。 一旦索引请求成功返回给用户，文档在主分片和副本分片都是可用的。

## 局部更新文档

如 [Figure 11, “局部更新文档”]所示，`update` API 结合了先前说明的读取和写入模式。

![image-20210914155308527](interviewImg\image-20210914155308527.png)

以下是部分更新一个文档的步骤：

1. 客户端向 `Node 1` 发送更新请求。
2. 它将请求转发到主分片所在的 `Node 3` 。
3. `Node 3` 从主分片检索文档，修改 `_source` 字段中的 JSON ，并且尝试重新索引主分片的文档。 如果文档已经被另一个进程修改，它会重试步骤 3 ，超过 `retry_on_conflict` 次后放弃。
4. 如果 `Node 3` 成功地更新文档，它将新版本的文档并行转发到 `Node 1` 和 `Node 2` 上的副本分片，重新建立索引。 一旦所有副本分片都返回成功， `Node 3` 向协调节点也返回成功，协调节点向客户端返回成功。

> 基于文档的复制
>
> 当主分片把更改转发到副本分片时， 它不会转发更新请求。 相反，它转发完整文档的新版本。请记住，这些更改将会异步转发到副本分片，并且不能保证它们以发送它们相同的顺序到达。 如果Elasticsearch仅转发更改请求，则可能以错误的顺序应用更改，导致得到损坏的文档。

## 多文档模式

`mget` 和 `bulk` API 的模式类似于单文档模式。区别在于协调节点知道每个文档存在于哪个分片中。 它将整个多文档请求分解成 *每个分片* 的多文档请求，并且将这些请求并行转发到每个参与节点。

协调节点一旦收到来自每个节点的应答，就将每个节点的响应收集整理成单个响应，返回给客户端，如 [Figure 12, “使用 `mget` 取回多个文档”] 所示。

![image-20210914155605226](interviewImg\image-20210914155605226.png)

以下是使用单个 `mget` 请求取回多个文档所需的步骤顺序：

1. 客户端向 `Node 1` 发送 `mget` 请求。
2. `Node 1` 为每个分片构建多文档获取请求，然后并行转发这些请求到托管在每个所需的主分片或者副本分片的节点上。一旦收到所有答复， `Node 1` 构建响应并将其返回给客户端。

可以对 `docs` 数组中每个文档设置 `routing` 参数。

bulk API， 如 [Figure 13, “使用 `bulk` 修改多个文档”] 所示， 允许在单个批量请求中执行多个创建、索引、删除和更新请求。

![image-20210914155809217](interviewImg\image-20210914155809217.png)

`bulk` API 按如下步骤顺序执行：

1. 客户端向 `Node 1` 发送 `bulk` 请求。
2. `Node 1` 为每个节点创建一个批量请求，并将这些请求并行转发到每个包含主分片的节点主机。
3. 主分片一个接一个按顺序执行每个操作。当每个操作成功时，主分片并行转发新文档（或删除）到副本分片，然后执行下一个操作。 一旦所有的副本分片报告所有操作成功，该节点将向协调节点报告成功，协调节点将这些响应收集整理并返回给客户端。

`bulk` API 还可以在整个批量请求的最顶层使用 `consistency` 参数，以及在每个请求中的元数据中使用 `routing` 参数。

# 分布式检索

一个 CRUD 操作只对单个文档进行处理，文档的唯一性由 `_index`, `_type`, 和 routing` values（通常默认是该文档的 `_id` ）的组合来确定。 这表示我们确切的知道集群中哪个分片含有此文档。

搜索需要一种更加复杂的执行模型因为我们不知道查询会命中哪些文档: 这些文档有可能在集群的任何分片上。 一个搜索请求必须询问我们关注的索引（index or indices）的所有分片的某个副本来确定它们是否含有任何匹配的文档。

但是找到所有的匹配文档仅仅完成事情的一半。 在 `search` 接口返回一个 `page` 结果之前，多分片中的结果必须组合成单个排序列表。 为此，**搜索被执行成一个两阶段过程，我们称之为 *query then fetch* **。

## 查询阶段

在初始 *查询阶段* 时， **查询会广播到索引中每一个分片拷贝（主分片或者副本分片）**。 每个分片在本地执行搜索并构建一个匹配文档的 *优先队列*。

> 一个 *优先队列* 仅仅是一个存有 *top-n* 匹配文档的有序列表。优先队列的大小取决于分页参数 `from` 和 `size` 。例如，如下搜索请求将需要足够大的优先队列来放入100条文档。

![image-20210914161550683](C:\Users\18896\Desktop\JavaInterview\interviewImg\image-20210914161550683.png)

查询阶段包含以下三个步骤:

1. 客户端发送一个 `search` 请求到 `Node 3` ， `Node 3` 会创建一个大小为 `from + size` 的空优先队列。
2. `Node 3` 将查询请求转发到索引的每个主分片或副本分片中。每个分片在本地执行查询并添加结果到大小为 `from + size` 的本地有序优先队列中。
3. 每个分片返回各自优先队列中所有文档的 ID 和排序值给协调节点，也就是 `Node 3` ，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。











































